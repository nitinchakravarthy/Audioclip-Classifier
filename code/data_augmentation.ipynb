{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import hashlib\n",
    "import math\n",
    "import os.path\n",
    "import random\n",
    "import re\n",
    "import sys\n",
    "import tarfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.contrib.framework.python.ops import audio_ops as contrib_audio\n",
    "from tensorflow.python.ops import io_ops\n",
    "from tensorflow.python.platform import gfile\n",
    "from tensorflow.python.util import compat\n",
    "\n",
    "# If it's available, load the specialized feature generator. If this doesn't\n",
    "# work, try building with bazel instead of running the Python script directly.\n",
    "try:\n",
    "    from tensorflow.lite.experimental.microfrontend.python.ops import audio_microfrontend_op as frontend_op  # pylint:disable=g-import-not-at-top\n",
    "except ImportError:\n",
    "    frontend_op = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### properties for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SILENCE_LABEL = '_silence_'\n",
    "SILENCE_INDEX = 0\n",
    "UNKNOWN_WORD_LABEL = '_unknown_'\n",
    "UNKNOWN_WORD_INDEX = 1\n",
    "BACKGROUND_NOISE_DIR_NAME = '_background_noise_'\n",
    "RANDOM_SEED = 59185\n",
    "\n",
    "\n",
    "silence_percentage = 10\n",
    "unknown_percentage = 10\n",
    "\n",
    "sample_rate = 16000\n",
    "clip_duration_ms =1000\n",
    "window_size_ms = 30\n",
    "window_stride_ms = 10\n",
    "feature_bin_count = 40 \n",
    "time_shift_ms = 100.0\n",
    "background_volume_range = 0.1\n",
    "foreground_volume = 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_PATH = \"./speech/data/\"\n",
    "\n",
    "URBAN_NOISE_DATA_PATH = \"./urban/data/\"\n",
    "\n",
    "COMBINED_DATA_PATH = \"./combined/data/\"\n",
    "\n",
    "SPEECH_NPY_PATH = \"./data_aug/npy/\"\n",
    "\n",
    "URBAN_NOISE_NPY_PATH = \"./data_aug/npy/\"\n",
    "\n",
    "COMBINED_NPY_PATH =  \"./data_aug/npy/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "desired_samples = int(sample_rate * clip_duration_ms / 1000)\n",
    "window_size_samples = int(sample_rate * window_size_ms / 1000)\n",
    "window_stride_samples = int(sample_rate * window_stride_ms / 1000)\n",
    "length_minus_window = (desired_samples - window_size_samples)\n",
    "fingerprint_width = feature_bin_count\n",
    "\n",
    "time_shift_samples = int((time_shift_ms * sample_rate) / 1000)\n",
    "time_shift_amount = np.random.randint(-time_shift_samples, time_shift_samples)\n",
    "if time_shift_amount > 0:\n",
    "    time_shift_padding = [[time_shift_amount, 0], [0, 0]]\n",
    "    time_shift_offset = [0, 0]\n",
    "else:\n",
    "    time_shift_padding = [[0, -time_shift_amount], [0, 0]]\n",
    "    time_shift_offset = [-time_shift_amount, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(desired_samples)\n",
    "print(window_size_samples)\n",
    "print(window_stride_samples)\n",
    "print(length_minus_window)\n",
    "print(fingerprint_width)\n",
    "print(time_shift_samples)\n",
    "print(time_shift_amount)\n",
    "print(time_shift_padding)\n",
    "print(time_shift_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data = []\n",
    "background_dir = os.path.join(SPEECH_DATA_PATH ,BACKGROUND_NOISE_DIR_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preparing background data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_background_data():\n",
    "    # preparing backgound data\n",
    "    background_data = []\n",
    "    background_dir = os.path.join(SPEECH_DATA_PATH ,BACKGROUND_NOISE_DIR_NAME)\n",
    "    with tf.Session(graph=tf.Graph()) as sess:\n",
    "        wav_filename_placeholder = tf.placeholder(tf.string, [])\n",
    "        wav_loader = io_ops.read_file(wav_filename_placeholder)\n",
    "        wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1)\n",
    "        search_path = os.path.join(SPEECH_DATA_PATH ,BACKGROUND_NOISE_DIR_NAME,'*.wav')\n",
    "        for wav_path in gfile.Glob(search_path):\n",
    "            wav_data = sess.run(wav_decoder,feed_dict={wav_filename_placeholder: wav_path}).audio.flatten()\n",
    "            background_data.append(wav_data)\n",
    "    return background_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_data = prepare_background_data()\n",
    "print(len(background_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_index = np.random.randint(len(background_data))\n",
    "background_samples = background_data[background_index]\n",
    "background_offset = np.random.randint( 0, len(background_samples) - desired_samples)\n",
    "background_clipped = background_samples[background_offset:(background_offset + desired_samples)]\n",
    "background_reshaped = background_clipped.reshape([desired_samples, 1])\n",
    "background_volume = np.random.uniform(0, background_volume_range)\n",
    "background_data = background_reshaped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(background_index)\n",
    "print(background_samples)\n",
    "print(background_offset)\n",
    "print(background_clipped)\n",
    "print(background_reshaped)\n",
    "print(background_volume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loads a WAVE file, decodes it, scales the volume, shifts it in time, \n",
    "# adds in background noise, calculates a spectrogram, \n",
    "# and then builds an MFCC fingerprint from that.\n",
    "wav_filename = SPEECH_DATA_PATH + 'happy/27c30960_nohash_0.wav'\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to add augmentation and extract mfcc features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#does data augmentation with time shift, random background sounds\n",
    "\n",
    "# use this instead of wav2mfcc function\n",
    "\n",
    "def prepare_mfcc(wav_filename):\n",
    "    #placeholders for the session\n",
    "    wav_filename_placeholder_ = tf.placeholder(tf.string, [], name='wav_filename')\n",
    "    foreground_volume_placeholder_ = tf.placeholder(tf.float32, [], name='foreground_volume')\n",
    "    time_shift_padding_placeholder_ = tf.placeholder(tf.int32, [2, 2], name='time_shift_padding')\n",
    "    time_shift_offset_placeholder_ = tf.placeholder(tf.int32, [2], name='time_shift_offset')\n",
    "    background_data_placeholder_ = tf.placeholder(tf.float32, [desired_samples, 1], name='background_data')\n",
    "    background_volume_placeholder_ = tf.placeholder(tf.float32, [], name='background_volume')\\\n",
    "\n",
    "    wav_loader = io_ops.read_file(wav_filename_placeholder_)\n",
    "    wav_decoder = contrib_audio.decode_wav(wav_loader, desired_channels=1, desired_samples=desired_samples)\n",
    "    scaled_foreground = tf.multiply(wav_decoder.audio,foreground_volume_placeholder_)\n",
    "    # Shift the sample's start position, and pad any gaps with zeros.\n",
    "    padded_foreground = tf.pad(scaled_foreground,time_shift_padding_placeholder_,mode='CONSTANT')\n",
    "    sliced_foreground = tf.slice(padded_foreground,time_shift_offset_placeholder_,[desired_samples, -1])\n",
    "    # Mix in background noise.\n",
    "    background_mul = tf.multiply(background_data_placeholder_,background_volume_placeholder_)\n",
    "    background_add = tf.add(background_mul, sliced_foreground)\n",
    "    background_clamp = tf.clip_by_value(background_add, -1.0, 1.0)\n",
    "    # Run the spectrogram and MFCC ops to get a 2D 'fingerprint' of the audio.\n",
    "    spectrogram = contrib_audio.audio_spectrogram(background_clamp,\n",
    "          window_size=window_size_samples,\n",
    "          stride=window_stride_samples,\n",
    "          magnitude_squared=True)\n",
    "\n",
    "    tf.summary.image('spectrogram', tf.expand_dims(spectrogram, -1), max_outputs=1)\n",
    "    output_ = contrib_audio.mfcc(spectrogram, wav_decoder.sample_rate,dct_coefficient_count=fingerprint_width)\n",
    "    tf.summary.image('mfcc', tf.expand_dims(output_, -1), max_outputs=1)\n",
    "    input_dict = {\n",
    "        wav_filename_placeholder_: wav_filename,\n",
    "        time_shift_padding_placeholder_: time_shift_padding,\n",
    "        time_shift_offset_placeholder_: time_shift_offset,\n",
    "        background_data_placeholder_: background_data,\n",
    "        background_volume_placeholder_: background_volume,\n",
    "        foreground_volume_placeholder_: foreground_volume,\n",
    "    }\n",
    "    data_tensor = sess.run([output_], feed_dict=input_dict)\n",
    "    data = data_tensor[0].flatten()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    data = prepare_mfcc(wav_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.shape)\n",
    "spectrogram_length = 1 + int(length_minus_window / window_stride_samples)\n",
    "data = data.reshape(spectrogram_length, feature_bin_count)\n",
    "print(data.shape)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### prepare and save features of the augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the save data code here....with test train and val split\n",
    "# make sure the background_noise folder is not in the path\n",
    "def save_data_speech(path = SPEECH_DATA_PATH, testFile = testFile, valFile = valFile, max_len = 11,savepath = SPEECH_NPY_PATH):\n",
    "\n",
    "    test_file = open(testFile, \"r\")\n",
    "    testFilesList = test_file.read().split('\\n')\n",
    "\n",
    "    val_file = open(valFile, \"r\")\n",
    "    valFilesList = val_file.read().split('\\n')\n",
    "\n",
    "    #print(testFilesList)\n",
    "    #print(valFilesList)\n",
    "    labels,_,_ = get_labels(path)\n",
    "    print(labels)\n",
    "    for label in labels:\n",
    "        mfcc_train = []\n",
    "        mfcc_test = []\n",
    "        mfcc_val = []\n",
    "        # saving a tuple of wavfile path and label/name format to compare in the test and val list\n",
    "        wavfiles = [(path + label + '/' + wavfile, label + '/' + wavfile)\n",
    "                    for wavfile in os.listdir(path + '/' + label)]\n",
    "        \n",
    "        #print(wavfiles)\n",
    "        \n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            #print(wavfile[0])\n",
    "            #print(wavfile[1])\n",
    "            #mfcc = wav2mfcc(wavfile[0], max_len=max_len)\n",
    "            mfcc = prepare_mfcc(wavfile[0])\n",
    "            if wavfile[1] in testFilesList:\n",
    "                mfcc_test.append(mfcc)\n",
    "            elif wavfile[1] in valFilesList:\n",
    "                mfcc_val.append(mfcc)\n",
    "            else:\n",
    "                mfcc_train.append(mfcc)\n",
    "                \n",
    "        np.save(savepath + label + '_test.npy', mfcc_test)\n",
    "        np.save(savepath + label + '_val.npy', mfcc_val)\n",
    "        np.save(savepath + label + '_train.npy', mfcc_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just saving the urban data as the npy file.\n",
    "# will split the data into test train and val after loading the data nd the labels by using a test_train split function\n",
    "def save_urbanNoise_data(path = URBAN_NOISE_DATA_PATH, max_len = 11, savePath = URBAN_NOISE_NPY_PATH):\n",
    "    labels,_,_ = get_labels(path)\n",
    "    for label in labels:\n",
    "        mfccs = []\n",
    "        mfcc_train = []\n",
    "        mfcc_test = []\n",
    "        mfcc_val = []\n",
    "        print(label)\n",
    "        \n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        \n",
    "        for wavfile in tqdm(wavfiles, \"saving vectors of label - '{}'\".format(label)):\n",
    "            try:\n",
    "                mfcc = prepare_mfcc(wavfile, max_len = max_len)\n",
    "                mfccs.append(mfcc)\n",
    "            except:\n",
    "                print(wavfile)\n",
    "        \n",
    "        np.save(savePath + label + '.npy', mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech data\n",
    "with tf.Session() as sess:\n",
    "    save_data_speech()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#urban Noise data\n",
    "with tf.Session() as sess:\n",
    "    save_urbanNoise_data()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
