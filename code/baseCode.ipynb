{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "\n",
    "import librosa\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from preprocess import *\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for the next part to run the data needs to be extracted and placed in the folders as SPEECH_DATAA_PATH and URBAN_NOISE_DATA_PATH locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPEECH_DATA_PATH = \"./speech/data/\"\n",
    "\n",
    "URBAN_NOISE_DATA_PATH = \"./urban/data/\"\n",
    "\n",
    "COMBINED_DATA_PATH = \"./combined/data/\"\n",
    "\n",
    "SPEECH_NPY_PATH = \"./15bins/npy/\"\n",
    "\n",
    "URBAN_NOISE_NPY_PATH = \"./15bins/npy/\"\n",
    "\n",
    "COMBINED_NPY_PATH =  \"./combined/npy/\"\n",
    "\n",
    "COMBINED_MODEL_SAVE_PATH = \"./models\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# speech test and val file paths\n",
    "testFile = \"./data/testing_list.txt\"\n",
    "valFile = \"./data/validation_list.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to convert wav file to mfccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "##function to convert wav2mfcc\n",
    "# samp rate of the speech wav files is 16000\n",
    "# samp rate of the urban noise wav files is 8000\n",
    "# use the samprate argument to change that.... will downsample speech wav files to 8000 in the future to make it easier\n",
    "def wav2mfcc(file_path, max_len=11,samprate = 8000):\n",
    "    wave, sr = librosa.load(file_path, mono=True, sr=None)\n",
    "    wave = wave[::3]\n",
    "    ##mfcc = librosa.feature.mfcc(wave, sr=16000)\n",
    "    mfcc = librosa.feature.mfcc(wave, sr=samprate)\n",
    "    # If maximum length exceeds mfcc lengths then pad the remaining ones\n",
    "    if (max_len > mfcc.shape[1]):\n",
    "        pad_width = max_len - mfcc.shape[1]\n",
    "        mfcc = np.pad(mfcc, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "\n",
    "    # Else cutoff the remaining parts\n",
    "    else:\n",
    "        mfcc = mfcc[:, :max_len]\n",
    "    \n",
    "    return mfcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args: Folder Path\n",
    "# Output: Tuple (Label, Indices of the labels, one-hot encoded labels)\n",
    "# make sure the background noise folder is not insde the DATA_PATH. because that is used for data augmentation and not recognition\n",
    "def get_labels(path=COMBINED_DATA_PATH):\n",
    "    labels = os.listdir(path)\n",
    "    label_indices = np.arange(0, len(labels))\n",
    "    return labels, label_indices, to_categorical(label_indices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to extract and save speech data features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the save data code here....with test train and val split\n",
    "# make sure the background_noise folder is not in the path\n",
    "def save_data_speech(path = SPEECH_DATA_PATH, testFile = testFile, valFile = valFile, max_len = 11,savepath = SPEECH_NPY_PATH):\n",
    "\n",
    "    test_file = open(testFile, \"r\")\n",
    "    testFilesList = test_file.read().split('\\n')\n",
    "\n",
    "    val_file = open(valFile, \"r\")\n",
    "    valFilesList = val_file.read().split('\\n')\n",
    "\n",
    "    #print(testFilesList)\n",
    "    #print(valFilesList)\n",
    "    labels,_,_ = get_labels(path)\n",
    "    print(labels)\n",
    "    for label in labels:\n",
    "        mfcc_train = []\n",
    "        mfcc_test = []\n",
    "        mfcc_val = []\n",
    "        # saving a tuple of wavfile path and label/name format to compare in the test and val list\n",
    "        wavfiles = [(path + label + '/' + wavfile, label + '/' + wavfile)\n",
    "                    for wavfile in os.listdir(path + '/' + label)]\n",
    "        \n",
    "        #print(wavfiles)\n",
    "        \n",
    "        for wavfile in tqdm(wavfiles, \"Saving vectors of label - '{}'\".format(label)):\n",
    "            #print(wavfile[0])\n",
    "            #print(wavfile[1])\n",
    "            mfcc = wav2mfcc(wavfile[0], max_len=max_len)\n",
    "            if wavfile[1] in testFilesList:\n",
    "                mfcc_test.append(mfcc)\n",
    "            elif wavfile[1] in valFilesList:\n",
    "                mfcc_val.append(mfcc)\n",
    "            else:\n",
    "                mfcc_train.append(mfcc)\n",
    "                \n",
    "        np.save(savepath + label + '_test.npy', mfcc_test)\n",
    "        np.save(savepath + label + '_val.npy', mfcc_val)\n",
    "        np.save(savepath + label + '_train.npy', mfcc_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### function to extract and save urban sounds features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just saving the urban data as the npy file.\n",
    "# will split the data into test train and val after loading the data nd the labels by using a test_train split function\n",
    "def save_urbanNoise_data(path = URBAN_NOISE_DATA_PATH, max_len = 11, savePath = URBAN_NOISE_NPY_PATH):\n",
    "    labels,_,_ = get_labels(path)\n",
    "    for label in labels:\n",
    "        mfccs = []\n",
    "        mfcc_train = []\n",
    "        mfcc_test = []\n",
    "        mfcc_val = []\n",
    "        print(label)\n",
    "        \n",
    "        wavfiles = [path + label + '/' + wavfile for wavfile in os.listdir(path + '/' + label)]\n",
    "        \n",
    "        for wavfile in tqdm(wavfiles, \"saving vectors of label - '{}'\".format(label)):\n",
    "            try:\n",
    "                mfcc = wav2mfcc(wavfile, max_len = max_len)\n",
    "                mfccs.append(mfcc)\n",
    "            except:\n",
    "                print(wavfile)\n",
    "        \n",
    "        np.save(savePath + label + '.npy', mfccs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save data\n",
    "# Second dimension of the feature is dim2\n",
    "feature_dim_2 = 15      #max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# speech data\n",
    "save_data_speech(max_len = feature_dim_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#urban Noise data\n",
    "save_urbanNoise_data(max_len = feature_dim_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to import urban sounds data that was stored above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the urbanNoiseData along with their labels then split them into train,val and split and save as npy files\n",
    "# we will get all the data from all the classes append them into an array and then use a test train split function\n",
    "def getUrbanNoiseDataSplit(path = URBAN_NOISE_DATA_PATH, savePath = URBAN_NOISE_NPY_PATH, split_ratio=0.8,random_state = 42):\n",
    "    labels, indices, _ = get_labels(path)\n",
    "    # get the array for each label\n",
    "    X = np.load(savePath + labels[0] + '.npy')\n",
    "    Y = np.zeros(X.shape[0])\n",
    "\n",
    "    # Append all of the dataset into one single array, same goes for y\n",
    "    # filling with i+1 in each loop so basically starting the class values from 0 to 9.\n",
    "    for i, label in enumerate(labels[1:]):\n",
    "        x = np.load(savePath + label + '.npy')\n",
    "        X = np.vstack((X, x))\n",
    "        Y = np.append(Y, np.full(x.shape[0], fill_value= (i + 1)))\n",
    "\n",
    "    assert X.shape[0] == len(Y)\n",
    "    \n",
    "    # not shuffling the data for now. Will shuffle them while training\n",
    "    X_temp, X_test, Y_temp, Y_test = train_test_split(X, Y, test_size= (1 - split_ratio), random_state=random_state, shuffle=True)\n",
    "    X_train, X_val, Y_train, Y_val = train_test_split(X_temp,Y_temp, test_size = (1-split_ratio), random_state = random_state,shuffle = True)\n",
    "    \n",
    "    return labels,X_train,X_val,X_test,Y_train,Y_val,Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions to import speech commands data that was stored above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for speech we already split the data into train test and val and then saved the data. \n",
    "# so now we just need to import the data and then append all the labels\n",
    "def getSpeechDataSplit(path = SPEECH_DATA_PATH, savePath = SPEECH_NPY_PATH):\n",
    "    \n",
    "    labels, indices, _ = get_labels(path)\n",
    "    # get the array for each label\n",
    "    X_train = np.load(savePath + labels[0] + '_train.npy')\n",
    "    Y_train = np.zeros(X_train.shape[0])\n",
    "    X_val = np.load(savePath + labels[0] + '_val.npy')\n",
    "    Y_val = np.zeros(X_val.shape[0])\n",
    "    X_test = np.load(savePath + labels[0] + '_test.npy')\n",
    "    Y_test = np.zeros(X_test.shape[0])\n",
    "    \n",
    "    for i,label in enumerate(labels[1:]):\n",
    "        x_train = np.load(savePath + label + '_train.npy')\n",
    "        x_val = np.load(savePath + label + '_val.npy')\n",
    "        x_test = np.load(savePath + label + '_test.npy')\n",
    "        X_train = np.vstack((X_train, x_train))\n",
    "        X_val = np.vstack((X_val, x_val))\n",
    "        X_test = np.vstack((X_test, x_test))\n",
    "        \n",
    "        Y_train = np.append(Y_train, np.full(x_train.shape[0],fill_value= (i+1)))\n",
    "        Y_val = np.append(Y_val, np.full(x_val.shape[0],fill_value= (i+1)))\n",
    "        Y_test = np.append(Y_test, np.full(x_test.shape[0],fill_value= (i+1)))\n",
    "    \n",
    "        \n",
    "    return labels,X_train,X_val,X_test,Y_train,Y_val,Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['air_conditioner', 'dog_bark', 'street_music', 'car_horn', 'drilling', 'children_playing', 'siren', 'engine_idling', 'jackhammer', 'gun_shot']\n"
     ]
    }
   ],
   "source": [
    "# # Loading train set and test set of urban Noise\n",
    "labels_un, X_train_un,X_val_un, X_test_un, Y_train_un,Y_val_un, Y_test_un = getUrbanNoiseDataSplit()\n",
    "print(labels_un)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['down', 'learn', 'right', 'nine', 'eight', 'dog', 'bird', 'house', 'marvin', 'zero', 'sheila', 'bed', 'follow', 'off', 'happy', 'backward', 'on', 'cat', 'left', 'five', 'visual', 'one', 'no', 'two', 'yes', 'forward', 'tree', 'three', 'go', 'seven', 'six', 'wow', 'stop', 'four', 'up']\n"
     ]
    }
   ],
   "source": [
    "# # Loading train set and test set of speech\n",
    "labels_sp, X_train_sp,X_val_sp, X_test_sp, Y_train_sp,Y_val_sp, Y_test_sp = getSpeechDataSplit()\n",
    "print(labels_sp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5587, 20, 15), (1397, 20, 15), (1746, 20, 15), (5587,), (1397,), (1746,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print shapes of all arrays\n",
    "X_train_un.shape,X_val_un.shape, X_test_un.shape, Y_train_un.shape,Y_val_un.shape, Y_test_un.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((84843, 20, 15), (9981, 20, 15), (11005, 20, 15), (84843,), (9981,), (11005,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_sp.shape,X_val_sp.shape, X_test_sp.shape, Y_train_sp.shape,Y_val_sp.shape, Y_test_sp.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mergeing datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n"
     ]
    }
   ],
   "source": [
    "# if we want to combine both the samples we just need to add the Y sample with the numeber of labels in the previous dataset\n",
    "\n",
    "# combining samples\n",
    "labels = labels_sp + labels_un\n",
    "print(len(labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we add 35 to the urban Noise Y data to make them into the new labels\n",
    "\n",
    "\n",
    "Y_train_un_new = Y_train_un + 35\n",
    "Y_test_un_new = Y_test_un + 35\n",
    "Y_val_un_new = Y_val_un + 35\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90430,)\n",
      "(12751,)\n",
      "(11378,)\n"
     ]
    }
   ],
   "source": [
    "Y_train = np.append(Y_train_sp, Y_train_un_new)\n",
    "Y_test = np.append(Y_test_sp, Y_test_un_new)\n",
    "Y_val = np.append(Y_val_sp, Y_val_un_new)\n",
    "\n",
    "print(Y_train.shape)\n",
    "print(Y_test.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0. ... 39. 37. 35.]\n",
      "[ 0.  0.  0. ... 37. 38. 42.]\n",
      "[ 0.  0.  0. ... 39. 36. 41.]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)\n",
    "print(Y_train)\n",
    "print(Y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90430, 20, 15)\n",
      "(12751, 20, 15)\n",
      "(11378, 20, 15)\n"
     ]
    }
   ],
   "source": [
    "X_train = np.vstack((X_train_sp, X_train_un))\n",
    "X_test = np.vstack((X_test_sp, X_test_un))\n",
    "X_val = np.vstack((X_val_sp, X_val_un))\n",
    "\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have combined data set and seperate datasets\n",
    "# X_train_un, X_test_un, X_val_un, Y_train_un, Y_test_un, Y_val_un ---urban Noise data - 10 clasees - (0-9)\n",
    "# X_train_sp, X_test_sp, X_val_sp, Y_train_sp, Y_test_sp, Y_val_sp --- speech data - 35 classes - (0-34)\n",
    "# X_train, X_test, X_val, Y_train, Y_test, Y_val --- combined dtat - 45 classes (0-44)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Feature dimension\n",
    "feature_dim_1 = 20\n",
    "channel = 1\n",
    "epochs = 200\n",
    "batch_size = 100\n",
    "verbose = 1\n",
    "num_classes = 45    #keeps changing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to perform 2D convolution\n",
    "# all data\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_val = X_val.reshape(X_val.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "X_test = X_test.reshape(X_test.shape[0], feature_dim_1, feature_dim_2, channel)\n",
    "\n",
    "Y_train_hot = to_categorical(Y_train)\n",
    "Y_val_hot = to_categorical(Y_val)\n",
    "Y_test_hot = to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(90430, 20, 15, 1)\n",
      "(12751, 20, 15, 1)\n",
      "(11378, 20, 15, 1)\n",
      "(90430, 45)\n",
      "(12751, 45)\n",
      "(11378, 45)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_val.shape)\n",
    "\n",
    "print(Y_train_hot.shape)\n",
    "print(Y_test_hot.shape)\n",
    "print(Y_val_hot.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(feature_dim_1, feature_dim_2, channel)))\n",
    "    model.add(Conv2D(48, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(Conv2D(120, kernel_size=(2, 2), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=keras.optimizers.Adadelta(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/tintin/miniconda3/envs/tf36/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /home/tintin/miniconda3/envs/tf36/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From /home/tintin/miniconda3/envs/tf36/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 90430 samples, validate on 12751 samples\n",
      "Epoch 1/200\n",
      "90430/90430 [==============================] - 28s 308us/step - loss: 2.7326 - acc: 0.2422 - val_loss: 1.5159 - val_acc: 0.5611\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.56113, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-01-0.56.hdf5\n",
      "Epoch 2/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 1.6099 - acc: 0.5275 - val_loss: 1.1440 - val_acc: 0.6600\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.56113 to 0.66003, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-02-0.66.hdf5\n",
      "Epoch 3/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 1.3097 - acc: 0.6163 - val_loss: 1.0568 - val_acc: 0.6847\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.66003 to 0.68465, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-03-0.68.hdf5\n",
      "Epoch 4/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 1.1649 - acc: 0.6597 - val_loss: 0.9357 - val_acc: 0.7231\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.68465 to 0.72308, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-04-0.72.hdf5\n",
      "Epoch 5/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 1.0622 - acc: 0.6914 - val_loss: 0.9196 - val_acc: 0.7262\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.72308 to 0.72622, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-05-0.73.hdf5\n",
      "Epoch 6/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.9906 - acc: 0.7123 - val_loss: 0.9105 - val_acc: 0.7242\n",
      "\n",
      "Epoch 00006: val_acc did not improve from 0.72622\n",
      "Epoch 7/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.9277 - acc: 0.7308 - val_loss: 0.8475 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00007: val_acc improved from 0.72622 to 0.74606, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-07-0.75.hdf5\n",
      "Epoch 8/200\n",
      "90430/90430 [==============================] - 9s 95us/step - loss: 0.8774 - acc: 0.7420 - val_loss: 0.8295 - val_acc: 0.7562\n",
      "\n",
      "Epoch 00008: val_acc improved from 0.74606 to 0.75618, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-08-0.76.hdf5\n",
      "Epoch 9/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.8446 - acc: 0.7523 - val_loss: 0.8215 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00009: val_acc improved from 0.75618 to 0.76104, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-09-0.76.hdf5\n",
      "Epoch 10/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.8108 - acc: 0.7624 - val_loss: 0.8603 - val_acc: 0.7476\n",
      "\n",
      "Epoch 00010: val_acc did not improve from 0.76104\n",
      "Epoch 11/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.7841 - acc: 0.7708 - val_loss: 0.8279 - val_acc: 0.7644\n",
      "\n",
      "Epoch 00011: val_acc improved from 0.76104 to 0.76441, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-11-0.76.hdf5\n",
      "Epoch 12/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.7597 - acc: 0.7781 - val_loss: 0.8278 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00012: val_acc did not improve from 0.76441\n",
      "Epoch 13/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.7358 - acc: 0.7845 - val_loss: 0.8265 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00013: val_acc did not improve from 0.76441\n",
      "Epoch 14/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 0.7157 - acc: 0.7910 - val_loss: 0.8236 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00014: val_acc improved from 0.76441 to 0.76974, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-14-0.77.hdf5\n",
      "Epoch 15/200\n",
      "90430/90430 [==============================] - 9s 94us/step - loss: 0.6995 - acc: 0.7955 - val_loss: 0.8434 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00015: val_acc did not improve from 0.76974\n",
      "Epoch 16/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.6909 - acc: 0.7976 - val_loss: 0.8451 - val_acc: 0.7641\n",
      "\n",
      "Epoch 00016: val_acc did not improve from 0.76974\n",
      "Epoch 17/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.6668 - acc: 0.8048 - val_loss: 0.8186 - val_acc: 0.7787\n",
      "\n",
      "Epoch 00017: val_acc improved from 0.76974 to 0.77868, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-17-0.78.hdf5\n",
      "Epoch 18/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.6559 - acc: 0.8084 - val_loss: 0.8230 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00018: val_acc did not improve from 0.77868\n",
      "Epoch 19/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.6461 - acc: 0.8122 - val_loss: 0.8603 - val_acc: 0.7687\n",
      "\n",
      "Epoch 00019: val_acc did not improve from 0.77868\n",
      "Epoch 20/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.6473 - acc: 0.8116 - val_loss: 0.8430 - val_acc: 0.7726\n",
      "\n",
      "Epoch 00020: val_acc did not improve from 0.77868\n",
      "Epoch 21/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.6325 - acc: 0.8148 - val_loss: 0.8547 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00021: val_acc did not improve from 0.77868\n",
      "Epoch 22/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.6227 - acc: 0.8195 - val_loss: 0.8272 - val_acc: 0.7710\n",
      "\n",
      "Epoch 00022: val_acc did not improve from 0.77868\n",
      "Epoch 23/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.6183 - acc: 0.8192 - val_loss: 0.8350 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00023: val_acc did not improve from 0.77868\n",
      "Epoch 24/200\n",
      "90430/90430 [==============================] - 8s 88us/step - loss: 0.6046 - acc: 0.8234 - val_loss: 0.8484 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00024: val_acc did not improve from 0.77868\n",
      "Epoch 25/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.6093 - acc: 0.8230 - val_loss: 0.8546 - val_acc: 0.7737\n",
      "\n",
      "Epoch 00025: val_acc did not improve from 0.77868\n",
      "Epoch 26/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.5988 - acc: 0.8246 - val_loss: 0.8560 - val_acc: 0.7610\n",
      "\n",
      "Epoch 00026: val_acc did not improve from 0.77868\n",
      "Epoch 27/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5953 - acc: 0.8264 - val_loss: 0.8457 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00027: val_acc did not improve from 0.77868\n",
      "Epoch 28/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.5930 - acc: 0.8289 - val_loss: 0.8430 - val_acc: 0.7636\n",
      "\n",
      "Epoch 00028: val_acc did not improve from 0.77868\n",
      "Epoch 29/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5902 - acc: 0.8314 - val_loss: 0.8873 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00029: val_acc did not improve from 0.77868\n",
      "Epoch 30/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5807 - acc: 0.8313 - val_loss: 0.8787 - val_acc: 0.7506\n",
      "\n",
      "Epoch 00030: val_acc did not improve from 0.77868\n",
      "Epoch 31/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.5921 - acc: 0.8301 - val_loss: 0.8494 - val_acc: 0.7744\n",
      "\n",
      "Epoch 00031: val_acc did not improve from 0.77868\n",
      "Epoch 32/200\n",
      "90430/90430 [==============================] - 10s 113us/step - loss: 0.5819 - acc: 0.8306 - val_loss: 0.8571 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00032: val_acc did not improve from 0.77868\n",
      "Epoch 33/200\n",
      "90430/90430 [==============================] - 10s 110us/step - loss: 0.5749 - acc: 0.8360 - val_loss: 0.8526 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00033: val_acc did not improve from 0.77868\n",
      "Epoch 34/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.5771 - acc: 0.8340 - val_loss: 0.8860 - val_acc: 0.7635\n",
      "\n",
      "Epoch 00034: val_acc did not improve from 0.77868\n",
      "Epoch 35/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.5734 - acc: 0.8360 - val_loss: 0.9382 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00035: val_acc did not improve from 0.77868\n",
      "Epoch 36/200\n",
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.5749 - acc: 0.8348 - val_loss: 0.8939 - val_acc: 0.7461\n",
      "\n",
      "Epoch 00036: val_acc did not improve from 0.77868\n",
      "Epoch 37/200\n",
      "90430/90430 [==============================] - 10s 111us/step - loss: 0.5645 - acc: 0.8375 - val_loss: 0.9569 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00037: val_acc did not improve from 0.77868\n",
      "Epoch 38/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5601 - acc: 0.8399 - val_loss: 0.8893 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00038: val_acc did not improve from 0.77868\n",
      "Epoch 39/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.5573 - acc: 0.8409 - val_loss: 0.8603 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00039: val_acc did not improve from 0.77868\n",
      "Epoch 40/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.5592 - acc: 0.8410 - val_loss: 0.8602 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00040: val_acc did not improve from 0.77868\n",
      "Epoch 41/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.5576 - acc: 0.8422 - val_loss: 0.8654 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00041: val_acc did not improve from 0.77868\n",
      "Epoch 42/200\n",
      "90430/90430 [==============================] - 10s 110us/step - loss: 0.5556 - acc: 0.8423 - val_loss: 0.9071 - val_acc: 0.7414\n",
      "\n",
      "Epoch 00042: val_acc did not improve from 0.77868\n",
      "Epoch 43/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.5577 - acc: 0.8409 - val_loss: 0.9083 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00043: val_acc did not improve from 0.77868\n",
      "Epoch 44/200\n",
      "90430/90430 [==============================] - 8s 90us/step - loss: 0.5485 - acc: 0.8433 - val_loss: 0.9094 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00044: val_acc did not improve from 0.77868\n",
      "Epoch 45/200\n",
      "90430/90430 [==============================] - 8s 87us/step - loss: 0.5484 - acc: 0.8438 - val_loss: 0.9424 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00045: val_acc did not improve from 0.77868\n",
      "Epoch 46/200\n",
      "90430/90430 [==============================] - 8s 89us/step - loss: 0.5497 - acc: 0.8456 - val_loss: 0.9021 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00046: val_acc did not improve from 0.77868\n",
      "Epoch 47/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 0.5425 - acc: 0.8452 - val_loss: 0.8754 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00047: val_acc did not improve from 0.77868\n",
      "Epoch 48/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5448 - acc: 0.8447 - val_loss: 0.9573 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00048: val_acc did not improve from 0.77868\n",
      "Epoch 49/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5382 - acc: 0.8453 - val_loss: 1.0194 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00049: val_acc did not improve from 0.77868\n",
      "Epoch 50/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.5319 - acc: 0.8493 - val_loss: 0.8952 - val_acc: 0.7508\n",
      "\n",
      "Epoch 00050: val_acc did not improve from 0.77868\n",
      "Epoch 51/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.5302 - acc: 0.8507 - val_loss: 0.9680 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00051: val_acc improved from 0.77868 to 0.77986, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-51-0.78.hdf5\n",
      "Epoch 52/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.5344 - acc: 0.8482 - val_loss: 0.9146 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00052: val_acc did not improve from 0.77986\n",
      "Epoch 53/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5384 - acc: 0.8481 - val_loss: 0.9538 - val_acc: 0.7730\n",
      "\n",
      "Epoch 00053: val_acc did not improve from 0.77986\n",
      "Epoch 54/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.5319 - acc: 0.8509 - val_loss: 0.8951 - val_acc: 0.7557\n",
      "\n",
      "Epoch 00054: val_acc did not improve from 0.77986\n",
      "Epoch 55/200\n",
      "90430/90430 [==============================] - 8s 90us/step - loss: 0.5260 - acc: 0.8512 - val_loss: 0.9069 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00055: val_acc did not improve from 0.77986\n",
      "Epoch 56/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5378 - acc: 0.8484 - val_loss: 0.8987 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00056: val_acc did not improve from 0.77986\n",
      "Epoch 57/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.5287 - acc: 0.8512 - val_loss: 0.9025 - val_acc: 0.7483\n",
      "\n",
      "Epoch 00057: val_acc did not improve from 0.77986\n",
      "Epoch 58/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5256 - acc: 0.8511 - val_loss: 0.9518 - val_acc: 0.7565\n",
      "\n",
      "Epoch 00058: val_acc did not improve from 0.77986\n",
      "Epoch 59/200\n",
      "90430/90430 [==============================] - 8s 84us/step - loss: 0.5322 - acc: 0.8502 - val_loss: 1.0366 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00059: val_acc did not improve from 0.77986\n",
      "Epoch 60/200\n",
      "90430/90430 [==============================] - 8s 90us/step - loss: 0.5253 - acc: 0.8511 - val_loss: 0.9607 - val_acc: 0.7790\n",
      "\n",
      "Epoch 00060: val_acc did not improve from 0.77986\n",
      "Epoch 61/200\n",
      "90430/90430 [==============================] - 9s 94us/step - loss: 0.5212 - acc: 0.8549 - val_loss: 0.9671 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00061: val_acc did not improve from 0.77986\n",
      "Epoch 62/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5294 - acc: 0.8519 - val_loss: 0.9136 - val_acc: 0.7617\n",
      "\n",
      "Epoch 00062: val_acc did not improve from 0.77986\n",
      "Epoch 63/200\n",
      "90430/90430 [==============================] - 8s 85us/step - loss: 0.5215 - acc: 0.8539 - val_loss: 0.9495 - val_acc: 0.7648\n",
      "\n",
      "Epoch 00063: val_acc did not improve from 0.77986\n",
      "Epoch 64/200\n",
      "90430/90430 [==============================] - 8s 83us/step - loss: 0.5225 - acc: 0.8533 - val_loss: 0.9378 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00064: val_acc improved from 0.77986 to 0.78072, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-64-0.78.hdf5\n",
      "Epoch 65/200\n",
      "90430/90430 [==============================] - 8s 86us/step - loss: 0.5215 - acc: 0.8536 - val_loss: 0.8899 - val_acc: 0.7722\n",
      "\n",
      "Epoch 00065: val_acc did not improve from 0.78072\n",
      "Epoch 66/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5186 - acc: 0.8544 - val_loss: 0.8960 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00066: val_acc did not improve from 0.78072\n",
      "Epoch 67/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 0.5136 - acc: 0.8549 - val_loss: 0.9114 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00067: val_acc did not improve from 0.78072\n",
      "Epoch 68/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5210 - acc: 0.8542 - val_loss: 0.9798 - val_acc: 0.7683\n",
      "\n",
      "Epoch 00068: val_acc did not improve from 0.78072\n",
      "Epoch 69/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5135 - acc: 0.8565 - val_loss: 0.9165 - val_acc: 0.7434\n",
      "\n",
      "Epoch 00069: val_acc did not improve from 0.78072\n",
      "Epoch 70/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5181 - acc: 0.8554 - val_loss: 0.9192 - val_acc: 0.7469\n",
      "\n",
      "Epoch 00070: val_acc did not improve from 0.78072\n",
      "Epoch 71/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.5153 - acc: 0.8556 - val_loss: 0.9134 - val_acc: 0.7607\n",
      "\n",
      "Epoch 00071: val_acc did not improve from 0.78072\n",
      "Epoch 72/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 0.5148 - acc: 0.8552 - val_loss: 1.0465 - val_acc: 0.7779\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00072: val_acc did not improve from 0.78072\n",
      "Epoch 73/200\n",
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.5089 - acc: 0.8571 - val_loss: 0.9655 - val_acc: 0.7739\n",
      "\n",
      "Epoch 00073: val_acc did not improve from 0.78072\n",
      "Epoch 74/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.5164 - acc: 0.8570 - val_loss: 0.9630 - val_acc: 0.7303\n",
      "\n",
      "Epoch 00074: val_acc did not improve from 0.78072\n",
      "Epoch 75/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.5154 - acc: 0.8565 - val_loss: 0.9115 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00075: val_acc did not improve from 0.78072\n",
      "Epoch 76/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5124 - acc: 0.8575 - val_loss: 1.0202 - val_acc: 0.7697\n",
      "\n",
      "Epoch 00076: val_acc did not improve from 0.78072\n",
      "Epoch 77/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.5100 - acc: 0.8584 - val_loss: 0.9084 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00077: val_acc did not improve from 0.78072\n",
      "Epoch 78/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5133 - acc: 0.8566 - val_loss: 1.0099 - val_acc: 0.7693\n",
      "\n",
      "Epoch 00078: val_acc did not improve from 0.78072\n",
      "Epoch 79/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5103 - acc: 0.8584 - val_loss: 0.9072 - val_acc: 0.7572\n",
      "\n",
      "Epoch 00079: val_acc did not improve from 0.78072\n",
      "Epoch 80/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.5084 - acc: 0.8582 - val_loss: 0.9290 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00080: val_acc did not improve from 0.78072\n",
      "Epoch 81/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5127 - acc: 0.8589 - val_loss: 0.9008 - val_acc: 0.7630\n",
      "\n",
      "Epoch 00081: val_acc did not improve from 0.78072\n",
      "Epoch 82/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5026 - acc: 0.8602 - val_loss: 1.0660 - val_acc: 0.7759\n",
      "\n",
      "Epoch 00082: val_acc did not improve from 0.78072\n",
      "Epoch 83/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.5078 - acc: 0.8580 - val_loss: 0.9109 - val_acc: 0.7551\n",
      "\n",
      "Epoch 00083: val_acc did not improve from 0.78072\n",
      "Epoch 84/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5075 - acc: 0.8587 - val_loss: 0.9356 - val_acc: 0.7573\n",
      "\n",
      "Epoch 00084: val_acc did not improve from 0.78072\n",
      "Epoch 85/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.5096 - acc: 0.8600 - val_loss: 0.9477 - val_acc: 0.7513\n",
      "\n",
      "Epoch 00085: val_acc did not improve from 0.78072\n",
      "Epoch 86/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5067 - acc: 0.8579 - val_loss: 1.1157 - val_acc: 0.7757\n",
      "\n",
      "Epoch 00086: val_acc did not improve from 0.78072\n",
      "Epoch 87/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5044 - acc: 0.8594 - val_loss: 0.9563 - val_acc: 0.7634\n",
      "\n",
      "Epoch 00087: val_acc did not improve from 0.78072\n",
      "Epoch 88/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5059 - acc: 0.8584 - val_loss: 0.9939 - val_acc: 0.7774\n",
      "\n",
      "Epoch 00088: val_acc did not improve from 0.78072\n",
      "Epoch 89/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.5081 - acc: 0.8586 - val_loss: 1.1053 - val_acc: 0.7808\n",
      "\n",
      "Epoch 00089: val_acc improved from 0.78072 to 0.78080, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-89-0.78.hdf5\n",
      "Epoch 90/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.4999 - acc: 0.8605 - val_loss: 0.9081 - val_acc: 0.7645\n",
      "\n",
      "Epoch 00090: val_acc did not improve from 0.78080\n",
      "Epoch 91/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5073 - acc: 0.8592 - val_loss: 0.9467 - val_acc: 0.7650\n",
      "\n",
      "Epoch 00091: val_acc did not improve from 0.78080\n",
      "Epoch 92/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.5111 - acc: 0.8594 - val_loss: 1.0334 - val_acc: 0.7800\n",
      "\n",
      "Epoch 00092: val_acc did not improve from 0.78080\n",
      "Epoch 93/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5012 - acc: 0.8624 - val_loss: 1.0459 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00093: val_acc did not improve from 0.78080\n",
      "Epoch 94/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5057 - acc: 0.8601 - val_loss: 0.9558 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00094: val_acc did not improve from 0.78080\n",
      "Epoch 95/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5075 - acc: 0.8591 - val_loss: 1.0168 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00095: val_acc did not improve from 0.78080\n",
      "Epoch 96/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.5068 - acc: 0.8598 - val_loss: 1.0385 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00096: val_acc did not improve from 0.78080\n",
      "Epoch 97/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5028 - acc: 0.8611 - val_loss: 1.0452 - val_acc: 0.7729\n",
      "\n",
      "Epoch 00097: val_acc did not improve from 0.78080\n",
      "Epoch 98/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5022 - acc: 0.8617 - val_loss: 1.0628 - val_acc: 0.7807\n",
      "\n",
      "Epoch 00098: val_acc did not improve from 0.78080\n",
      "Epoch 99/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5010 - acc: 0.8608 - val_loss: 0.9677 - val_acc: 0.7756\n",
      "\n",
      "Epoch 00099: val_acc did not improve from 0.78080\n",
      "Epoch 100/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.5009 - acc: 0.8624 - val_loss: 1.0930 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00100: val_acc did not improve from 0.78080\n",
      "Epoch 101/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5089 - acc: 0.8596 - val_loss: 0.9356 - val_acc: 0.7545\n",
      "\n",
      "Epoch 00101: val_acc did not improve from 0.78080\n",
      "Epoch 102/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5069 - acc: 0.8595 - val_loss: 1.0129 - val_acc: 0.7748\n",
      "\n",
      "Epoch 00102: val_acc did not improve from 0.78080\n",
      "Epoch 103/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.5068 - acc: 0.8615 - val_loss: 0.9941 - val_acc: 0.7668\n",
      "\n",
      "Epoch 00103: val_acc did not improve from 0.78080\n",
      "Epoch 104/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.5001 - acc: 0.8613 - val_loss: 0.9816 - val_acc: 0.7637\n",
      "\n",
      "Epoch 00104: val_acc did not improve from 0.78080\n",
      "Epoch 105/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5112 - acc: 0.8604 - val_loss: 0.9982 - val_acc: 0.7764\n",
      "\n",
      "Epoch 00105: val_acc did not improve from 0.78080\n",
      "Epoch 106/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5068 - acc: 0.8617 - val_loss: 1.0518 - val_acc: 0.7814\n",
      "\n",
      "Epoch 00106: val_acc improved from 0.78080 to 0.78135, saving model to /media/tintin/3bde46c1-cca7-4db6-b2ca-e71397572866/tintin/speech-urban-data/combined/models/all_data-106-0.78.hdf5\n",
      "Epoch 107/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5042 - acc: 0.8602 - val_loss: 1.0125 - val_acc: 0.7659\n",
      "\n",
      "Epoch 00107: val_acc did not improve from 0.78135\n",
      "Epoch 108/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4921 - acc: 0.8628 - val_loss: 1.0608 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00108: val_acc did not improve from 0.78135\n",
      "Epoch 109/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5027 - acc: 0.8613 - val_loss: 0.9820 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00109: val_acc did not improve from 0.78135\n",
      "Epoch 110/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5079 - acc: 0.8611 - val_loss: 1.0511 - val_acc: 0.7734\n",
      "\n",
      "Epoch 00110: val_acc did not improve from 0.78135\n",
      "Epoch 111/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5016 - acc: 0.8628 - val_loss: 0.9324 - val_acc: 0.7502\n",
      "\n",
      "Epoch 00111: val_acc did not improve from 0.78135\n",
      "Epoch 112/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.5081 - acc: 0.8602 - val_loss: 1.0734 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00112: val_acc did not improve from 0.78135\n",
      "Epoch 113/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.4961 - acc: 0.8631 - val_loss: 1.0051 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00113: val_acc did not improve from 0.78135\n",
      "Epoch 114/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4980 - acc: 0.8632 - val_loss: 1.0486 - val_acc: 0.7604\n",
      "\n",
      "Epoch 00114: val_acc did not improve from 0.78135\n",
      "Epoch 115/200\n",
      "90430/90430 [==============================] - 10s 110us/step - loss: 0.5002 - acc: 0.8630 - val_loss: 0.9457 - val_acc: 0.7551\n",
      "\n",
      "Epoch 00115: val_acc did not improve from 0.78135\n",
      "Epoch 116/200\n",
      "90430/90430 [==============================] - 10s 110us/step - loss: 0.5019 - acc: 0.8614 - val_loss: 0.9526 - val_acc: 0.7422\n",
      "\n",
      "Epoch 00116: val_acc did not improve from 0.78135\n",
      "Epoch 117/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4970 - acc: 0.8638 - val_loss: 0.9512 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00117: val_acc did not improve from 0.78135\n",
      "Epoch 118/200\n",
      "90430/90430 [==============================] - 10s 111us/step - loss: 0.4984 - acc: 0.8632 - val_loss: 1.1095 - val_acc: 0.7628\n",
      "\n",
      "Epoch 00118: val_acc did not improve from 0.78135\n",
      "Epoch 119/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4938 - acc: 0.8635 - val_loss: 1.1709 - val_acc: 0.7766\n",
      "\n",
      "Epoch 00119: val_acc did not improve from 0.78135\n",
      "Epoch 120/200\n",
      "90430/90430 [==============================] - 10s 110us/step - loss: 0.5036 - acc: 0.8608 - val_loss: 0.9801 - val_acc: 0.7712\n",
      "\n",
      "Epoch 00120: val_acc did not improve from 0.78135\n",
      "Epoch 121/200\n",
      "90430/90430 [==============================] - 10s 109us/step - loss: 0.4920 - acc: 0.8657 - val_loss: 0.9234 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00121: val_acc did not improve from 0.78135\n",
      "Epoch 122/200\n",
      "90430/90430 [==============================] - 10s 112us/step - loss: 0.4885 - acc: 0.8646 - val_loss: 0.9559 - val_acc: 0.7556\n",
      "\n",
      "Epoch 00122: val_acc did not improve from 0.78135\n",
      "Epoch 123/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4989 - acc: 0.8637 - val_loss: 0.9763 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00123: val_acc did not improve from 0.78135\n",
      "Epoch 124/200\n",
      "90430/90430 [==============================] - 10s 109us/step - loss: 0.4972 - acc: 0.8629 - val_loss: 1.0422 - val_acc: 0.7649\n",
      "\n",
      "Epoch 00124: val_acc did not improve from 0.78135\n",
      "Epoch 125/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.4942 - acc: 0.8656 - val_loss: 0.9959 - val_acc: 0.7487\n",
      "\n",
      "Epoch 00125: val_acc did not improve from 0.78135\n",
      "Epoch 126/200\n",
      "90430/90430 [==============================] - 10s 111us/step - loss: 0.4948 - acc: 0.8636 - val_loss: 0.9555 - val_acc: 0.7590\n",
      "\n",
      "Epoch 00126: val_acc did not improve from 0.78135\n",
      "Epoch 127/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4940 - acc: 0.8650 - val_loss: 1.0690 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00127: val_acc did not improve from 0.78135\n",
      "Epoch 128/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4949 - acc: 0.8647 - val_loss: 0.9987 - val_acc: 0.7752\n",
      "\n",
      "Epoch 00128: val_acc did not improve from 0.78135\n",
      "Epoch 129/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.4894 - acc: 0.8658 - val_loss: 1.0035 - val_acc: 0.7718\n",
      "\n",
      "Epoch 00129: val_acc did not improve from 0.78135\n",
      "Epoch 130/200\n",
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.4939 - acc: 0.8642 - val_loss: 0.9643 - val_acc: 0.7713\n",
      "\n",
      "Epoch 00130: val_acc did not improve from 0.78135\n",
      "Epoch 131/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4914 - acc: 0.8656 - val_loss: 1.0677 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00131: val_acc did not improve from 0.78135\n",
      "Epoch 132/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4922 - acc: 0.8646 - val_loss: 1.2785 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00132: val_acc did not improve from 0.78135\n",
      "Epoch 133/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4976 - acc: 0.8634 - val_loss: 0.9418 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00133: val_acc did not improve from 0.78135\n",
      "Epoch 134/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.4939 - acc: 0.8654 - val_loss: 1.0290 - val_acc: 0.7516\n",
      "\n",
      "Epoch 00134: val_acc did not improve from 0.78135\n",
      "Epoch 135/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4976 - acc: 0.8652 - val_loss: 1.0426 - val_acc: 0.7676\n",
      "\n",
      "Epoch 00135: val_acc did not improve from 0.78135\n",
      "Epoch 136/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4969 - acc: 0.8654 - val_loss: 1.0857 - val_acc: 0.7300\n",
      "\n",
      "Epoch 00136: val_acc did not improve from 0.78135\n",
      "Epoch 137/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4931 - acc: 0.8657 - val_loss: 1.0141 - val_acc: 0.7717\n",
      "\n",
      "Epoch 00137: val_acc did not improve from 0.78135\n",
      "Epoch 138/200\n",
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.4966 - acc: 0.8641 - val_loss: 1.2491 - val_acc: 0.7695\n",
      "\n",
      "Epoch 00138: val_acc did not improve from 0.78135\n",
      "Epoch 139/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4941 - acc: 0.8655 - val_loss: 1.1933 - val_acc: 0.7720\n",
      "\n",
      "Epoch 00139: val_acc did not improve from 0.78135\n",
      "Epoch 140/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4903 - acc: 0.8665 - val_loss: 1.0198 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00140: val_acc did not improve from 0.78135\n",
      "Epoch 141/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4942 - acc: 0.8654 - val_loss: 1.1534 - val_acc: 0.7453\n",
      "\n",
      "Epoch 00141: val_acc did not improve from 0.78135\n",
      "Epoch 142/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4949 - acc: 0.8648 - val_loss: 1.0804 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00142: val_acc did not improve from 0.78135\n",
      "Epoch 143/200\n",
      "90430/90430 [==============================] - 10s 106us/step - loss: 0.4978 - acc: 0.8651 - val_loss: 1.0505 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00143: val_acc did not improve from 0.78135\n",
      "Epoch 144/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4942 - acc: 0.8656 - val_loss: 1.0085 - val_acc: 0.7276\n",
      "\n",
      "Epoch 00144: val_acc did not improve from 0.78135\n",
      "Epoch 145/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.5041 - acc: 0.8654 - val_loss: 1.0284 - val_acc: 0.7784\n",
      "\n",
      "Epoch 00145: val_acc did not improve from 0.78135\n",
      "Epoch 146/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4916 - acc: 0.8646 - val_loss: 0.9979 - val_acc: 0.7646\n",
      "\n",
      "Epoch 00146: val_acc did not improve from 0.78135\n",
      "Epoch 147/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.5002 - acc: 0.8640 - val_loss: 0.9569 - val_acc: 0.7578\n",
      "\n",
      "Epoch 00147: val_acc did not improve from 0.78135\n",
      "Epoch 148/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.4984 - acc: 0.8635 - val_loss: 0.9830 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00148: val_acc did not improve from 0.78135\n",
      "Epoch 149/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4942 - acc: 0.8650 - val_loss: 1.0896 - val_acc: 0.7699\n",
      "\n",
      "Epoch 00149: val_acc did not improve from 0.78135\n",
      "Epoch 150/200\n",
      "90430/90430 [==============================] - 10s 107us/step - loss: 0.4971 - acc: 0.8647 - val_loss: 1.0575 - val_acc: 0.7770\n",
      "\n",
      "Epoch 00150: val_acc did not improve from 0.78135\n",
      "Epoch 151/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.5005 - acc: 0.8649 - val_loss: 0.9706 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00151: val_acc did not improve from 0.78135\n",
      "Epoch 152/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4939 - acc: 0.8641 - val_loss: 1.0843 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00152: val_acc did not improve from 0.78135\n",
      "Epoch 153/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.4951 - acc: 0.8648 - val_loss: 0.9623 - val_acc: 0.7705\n",
      "\n",
      "Epoch 00153: val_acc did not improve from 0.78135\n",
      "Epoch 154/200\n",
      "90430/90430 [==============================] - 10s 105us/step - loss: 0.4981 - acc: 0.8657 - val_loss: 0.9462 - val_acc: 0.7445\n",
      "\n",
      "Epoch 00154: val_acc did not improve from 0.78135\n",
      "Epoch 155/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4965 - acc: 0.8649 - val_loss: 1.0819 - val_acc: 0.7617\n",
      "\n",
      "Epoch 00155: val_acc did not improve from 0.78135\n",
      "Epoch 156/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.5049 - acc: 0.8637 - val_loss: 0.9425 - val_acc: 0.7646\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00156: val_acc did not improve from 0.78135\n",
      "Epoch 157/200\n",
      "90430/90430 [==============================] - 10s 112us/step - loss: 0.4963 - acc: 0.8658 - val_loss: 1.0044 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00157: val_acc did not improve from 0.78135\n",
      "Epoch 158/200\n",
      "90430/90430 [==============================] - 10s 108us/step - loss: 0.4919 - acc: 0.8664 - val_loss: 1.1506 - val_acc: 0.7754\n",
      "\n",
      "Epoch 00158: val_acc did not improve from 0.78135\n",
      "Epoch 159/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4885 - acc: 0.8665 - val_loss: 1.0218 - val_acc: 0.7617\n",
      "\n",
      "Epoch 00159: val_acc did not improve from 0.78135\n",
      "Epoch 160/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.4982 - acc: 0.8635 - val_loss: 1.0737 - val_acc: 0.7701\n",
      "\n",
      "Epoch 00160: val_acc did not improve from 0.78135\n",
      "Epoch 161/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4979 - acc: 0.8647 - val_loss: 1.0870 - val_acc: 0.6948\n",
      "\n",
      "Epoch 00161: val_acc did not improve from 0.78135\n",
      "Epoch 162/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.5004 - acc: 0.8651 - val_loss: 1.0184 - val_acc: 0.7672\n",
      "\n",
      "Epoch 00162: val_acc did not improve from 0.78135\n",
      "Epoch 163/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4883 - acc: 0.8666 - val_loss: 1.0121 - val_acc: 0.7186\n",
      "\n",
      "Epoch 00163: val_acc did not improve from 0.78135\n",
      "Epoch 164/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4884 - acc: 0.8669 - val_loss: 1.2363 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00164: val_acc did not improve from 0.78135\n",
      "Epoch 165/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4992 - acc: 0.8651 - val_loss: 0.9569 - val_acc: 0.7585\n",
      "\n",
      "Epoch 00165: val_acc did not improve from 0.78135\n",
      "Epoch 166/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4918 - acc: 0.8677 - val_loss: 1.0818 - val_acc: 0.7708\n",
      "\n",
      "Epoch 00166: val_acc did not improve from 0.78135\n",
      "Epoch 167/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.4918 - acc: 0.8659 - val_loss: 0.9962 - val_acc: 0.7395\n",
      "\n",
      "Epoch 00167: val_acc did not improve from 0.78135\n",
      "Epoch 168/200\n",
      "90430/90430 [==============================] - 9s 104us/step - loss: 0.4944 - acc: 0.8672 - val_loss: 1.0203 - val_acc: 0.7703\n",
      "\n",
      "Epoch 00168: val_acc did not improve from 0.78135\n",
      "Epoch 169/200\n",
      "90430/90430 [==============================] - 9s 105us/step - loss: 0.4952 - acc: 0.8644 - val_loss: 1.0786 - val_acc: 0.7689\n",
      "\n",
      "Epoch 00169: val_acc did not improve from 0.78135\n",
      "Epoch 170/200\n",
      "90430/90430 [==============================] - 10s 109us/step - loss: 0.4994 - acc: 0.8652 - val_loss: 1.0286 - val_acc: 0.7719\n",
      "\n",
      "Epoch 00170: val_acc did not improve from 0.78135\n",
      "Epoch 171/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4949 - acc: 0.8649 - val_loss: 0.9575 - val_acc: 0.7538\n",
      "\n",
      "Epoch 00171: val_acc did not improve from 0.78135\n",
      "Epoch 172/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4975 - acc: 0.8650 - val_loss: 1.1343 - val_acc: 0.7664\n",
      "\n",
      "Epoch 00172: val_acc did not improve from 0.78135\n",
      "Epoch 173/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4946 - acc: 0.8662 - val_loss: 1.1400 - val_acc: 0.7799\n",
      "\n",
      "Epoch 00173: val_acc did not improve from 0.78135\n",
      "Epoch 174/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.4962 - acc: 0.8655 - val_loss: 1.0360 - val_acc: 0.7530\n",
      "\n",
      "Epoch 00174: val_acc did not improve from 0.78135\n",
      "Epoch 175/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4891 - acc: 0.8670 - val_loss: 1.0724 - val_acc: 0.7655\n",
      "\n",
      "Epoch 00175: val_acc did not improve from 0.78135\n",
      "Epoch 176/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4975 - acc: 0.8664 - val_loss: 1.0895 - val_acc: 0.7671\n",
      "\n",
      "Epoch 00176: val_acc did not improve from 0.78135\n",
      "Epoch 177/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4976 - acc: 0.8652 - val_loss: 1.1089 - val_acc: 0.7727\n",
      "\n",
      "Epoch 00177: val_acc did not improve from 0.78135\n",
      "Epoch 178/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.4995 - acc: 0.8657 - val_loss: 1.0157 - val_acc: 0.7723\n",
      "\n",
      "Epoch 00178: val_acc did not improve from 0.78135\n",
      "Epoch 179/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4993 - acc: 0.8639 - val_loss: 1.0244 - val_acc: 0.7606\n",
      "\n",
      "Epoch 00179: val_acc did not improve from 0.78135\n",
      "Epoch 180/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4933 - acc: 0.8646 - val_loss: 1.0319 - val_acc: 0.7514\n",
      "\n",
      "Epoch 00180: val_acc did not improve from 0.78135\n",
      "Epoch 181/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.4927 - acc: 0.8673 - val_loss: 1.1068 - val_acc: 0.7781\n",
      "\n",
      "Epoch 00181: val_acc did not improve from 0.78135\n",
      "Epoch 182/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.4948 - acc: 0.8660 - val_loss: 1.0651 - val_acc: 0.7700\n",
      "\n",
      "Epoch 00182: val_acc did not improve from 0.78135\n",
      "Epoch 183/200\n",
      "90430/90430 [==============================] - 8s 94us/step - loss: 0.4944 - acc: 0.8670 - val_loss: 1.1789 - val_acc: 0.7755\n",
      "\n",
      "Epoch 00183: val_acc did not improve from 0.78135\n",
      "Epoch 184/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.4993 - acc: 0.8647 - val_loss: 1.0510 - val_acc: 0.7615\n",
      "\n",
      "Epoch 00184: val_acc did not improve from 0.78135\n",
      "Epoch 185/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4999 - acc: 0.8661 - val_loss: 1.0284 - val_acc: 0.7627\n",
      "\n",
      "Epoch 00185: val_acc did not improve from 0.78135\n",
      "Epoch 186/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4983 - acc: 0.8646 - val_loss: 1.2155 - val_acc: 0.7786\n",
      "\n",
      "Epoch 00186: val_acc did not improve from 0.78135\n",
      "Epoch 187/200\n",
      "90430/90430 [==============================] - 9s 100us/step - loss: 0.4961 - acc: 0.8657 - val_loss: 0.9824 - val_acc: 0.7522\n",
      "\n",
      "Epoch 00187: val_acc did not improve from 0.78135\n",
      "Epoch 188/200\n",
      "90430/90430 [==============================] - 9s 102us/step - loss: 0.4973 - acc: 0.8642 - val_loss: 1.2420 - val_acc: 0.7728\n",
      "\n",
      "Epoch 00188: val_acc did not improve from 0.78135\n",
      "Epoch 189/200\n",
      "90430/90430 [==============================] - 9s 101us/step - loss: 0.4959 - acc: 0.8655 - val_loss: 1.0955 - val_acc: 0.7638\n",
      "\n",
      "Epoch 00189: val_acc did not improve from 0.78135\n",
      "Epoch 190/200\n",
      "90430/90430 [==============================] - 8s 92us/step - loss: 0.4947 - acc: 0.8657 - val_loss: 0.9913 - val_acc: 0.7559\n",
      "\n",
      "Epoch 00190: val_acc did not improve from 0.78135\n",
      "Epoch 191/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.5043 - acc: 0.8644 - val_loss: 1.1302 - val_acc: 0.7777\n",
      "\n",
      "Epoch 00191: val_acc did not improve from 0.78135\n",
      "Epoch 192/200\n",
      "90430/90430 [==============================] - 9s 98us/step - loss: 0.4960 - acc: 0.8665 - val_loss: 0.9893 - val_acc: 0.7477\n",
      "\n",
      "Epoch 00192: val_acc did not improve from 0.78135\n",
      "Epoch 193/200\n",
      "90430/90430 [==============================] - 9s 103us/step - loss: 0.5044 - acc: 0.8654 - val_loss: 1.0833 - val_acc: 0.7600\n",
      "\n",
      "Epoch 00193: val_acc did not improve from 0.78135\n",
      "Epoch 194/200\n",
      "90430/90430 [==============================] - 9s 97us/step - loss: 0.5049 - acc: 0.8644 - val_loss: 1.2031 - val_acc: 0.7577\n",
      "\n",
      "Epoch 00194: val_acc did not improve from 0.78135\n",
      "Epoch 195/200\n",
      "90430/90430 [==============================] - 8s 93us/step - loss: 0.4945 - acc: 0.8661 - val_loss: 0.9939 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00195: val_acc did not improve from 0.78135\n",
      "Epoch 196/200\n",
      "90430/90430 [==============================] - 9s 95us/step - loss: 0.4983 - acc: 0.8665 - val_loss: 1.0794 - val_acc: 0.7629\n",
      "\n",
      "Epoch 00196: val_acc did not improve from 0.78135\n",
      "Epoch 197/200\n",
      "90430/90430 [==============================] - 9s 96us/step - loss: 0.5013 - acc: 0.8654 - val_loss: 1.1189 - val_acc: 0.6900\n",
      "\n",
      "Epoch 00197: val_acc did not improve from 0.78135\n",
      "Epoch 198/200\n",
      "90430/90430 [==============================] - 9s 99us/step - loss: 0.4968 - acc: 0.8660 - val_loss: 0.9498 - val_acc: 0.7625\n",
      "\n",
      "Epoch 00198: val_acc did not improve from 0.78135\n",
      "Epoch 199/200\n",
      "90430/90430 [==============================] - 10s 111us/step - loss: 0.4920 - acc: 0.8673 - val_loss: 1.1300 - val_acc: 0.7694\n",
      "\n",
      "Epoch 00199: val_acc did not improve from 0.78135\n",
      "Epoch 200/200\n",
      "90430/90430 [==============================] - 10s 111us/step - loss: 0.5055 - acc: 0.8642 - val_loss: 1.0051 - val_acc: 0.7240\n",
      "\n",
      "Epoch 00200: val_acc did not improve from 0.78135\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "# checkpoint\n",
    "filepath= \"./models/baseline.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "\n",
    "history = model.fit(X_train, Y_train_hot, batch_size=batch_size, epochs=epochs, verbose=verbose, \n",
    "                    validation_data=(X_test, Y_test_hot), callbacks=callbacks_list, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loss and accuracy plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['val_loss', 'val_acc', 'loss', 'acc'])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# list all data in history\n",
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(COMBINED_MODEL_SAVE_PATH + 'model_accuracy.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.savefig(COMBINED_MODEL_SAVE_PATH + 'model_loss.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_22 (Conv2D)           (None, 19, 14, 32)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_23 (Conv2D)           (None, 18, 13, 48)        6192      \n",
      "_________________________________________________________________\n",
      "conv2d_24 (Conv2D)           (None, 17, 12, 120)       23160     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 8, 6, 120)         0         \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 8, 6, 120)         0         \n",
      "_________________________________________________________________\n",
      "flatten_8 (Flatten)          (None, 5760)              0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 128)               737408    \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 45)                2925      \n",
      "=================================================================\n",
      "Total params: 778,101\n",
      "Trainable params: 778,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# loading the model with the best val accuracy\n",
    "model = get_model()\n",
    "weights_path = \"./models/baseline.hdf5\"\n",
    "model.load_weights(weights_path)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90430/90430 [==============================] - 6s 66us/step\n",
      "11378/11378 [==============================] - 1s 66us/step\n",
      "12751/12751 [==============================] - 1s 66us/step\n",
      "[0.10196072650515217, 0.9709056728869351]\n",
      "[0.9778057609056626, 0.8036561785693076]\n",
      "[1.1269509643444817, 0.781977884120244]\n"
     ]
    }
   ],
   "source": [
    "#train and test loss and scores respectively\n",
    "train_loss_score=model.evaluate(X_train,Y_train_hot)\n",
    "val_loss_score=model.evaluate(X_val,Y_val_hot)\n",
    "test_loss_score=model.evaluate(X_test,Y_test_hot)\n",
    "print(train_loss_score)\n",
    "print(val_loss_score)\n",
    "print(test_loss_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicts one sample\n",
    "labels = ['down', 'learn', 'right', 'nine', 'eight', 'dog', 'bird', 'house',\n",
    " 'marvin', 'zero', 'sheila', 'bed', 'follow', 'off', 'happy', 'backward', 'on',\n",
    "  'cat', 'left', 'five', 'visual', 'one', 'no', 'two', 'yes', 'forward', 'tree',\n",
    "   'three', 'go', 'seven', 'six', 'wow', 'stop', 'four', 'up','air_conditioner',\n",
    "   'dog_bark', 'street_music', 'car_horn', 'drilling', 'children_playing',\n",
    "    'siren', 'engine_idling', 'jackhammer', 'gun_shot']\n",
    "\n",
    "def predict(filepath, model,labels):\n",
    "    mfcc = wav2mfcc(testFile, max_len=15)\n",
    "    mfcc_reshaped = mfcc.reshape(1,feature_dim_1,feature_dim_2,channel)\n",
    "    odds= model.predict(mfcc_reshaped)\n",
    "    odds_max = np.argmax(odds)\n",
    "    label = labels[odds_max]\n",
    "    return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "happy\n"
     ]
    }
   ],
   "source": [
    "testFile = SPEECH_DATA_PATH + 'happy/27c30960_nohash_0.wav'\n",
    "print(predict(testFile,model,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "feature_dim_2 = 15      #max_len\n",
    "feature_dim_1 = 20\n",
    "channel=1\n",
    "num_classes=45\n",
    "#max_len\n",
    "model = get_model()\n",
    "plot_model(model, show_shapes = True, show_layer_names = True,to_file='model.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
